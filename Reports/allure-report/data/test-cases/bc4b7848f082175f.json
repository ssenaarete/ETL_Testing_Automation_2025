{"uid":"bc4b7848f082175f","name":"test_ReferentialIntegrity_validation","fullName":"tests.test_smoke_suite#test_ReferentialIntegrity_validation","historyId":"3f9240f98ba6d75d4e2bc455e7b3a063","time":{"start":1759842547921,"stop":1759842570242,"duration":22321},"status":"failed","statusMessage":"AssertionError: ❌ Referential Integrity checks failed. See report.","statusTrace":"config_loader = <utils.config_loader.ConfigLoader object at 0x000001BE3CB6EBA0>\n\n    @pytest.mark.skipif(not should_run(\"Referential_Integrity_validation\"), reason=\"Marked N in Excel\")\n    @pytest.mark.not_for_source\n    def test_ReferentialIntegrity_validation(config_loader):\n        validator = ReferentialIntegrity_Validation(config_loader)\n>       validator.run()\n\ntests\\test_smoke_suite.py:109: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <src.Referential_Integrity_validation.ReferentialIntegrity_Validation object at 0x000001BE3CB6F230>\nschema = 'dbo'\n\n    def run(self, schema=\"dbo\"):\n        if self.excel_df is None or self.excel_df.empty:\n            logging.error(\"❌ Referential Integrity Check sheet is missing or empty in Excel.\")\n            return\n    \n        required_cols = {\"parent_table\", \"parent_column\", \"child_table\", \"child_column\"}\n        if not required_cols.issubset(set(self.excel_df.columns.str.lower())):\n            logging.error(f\"❌ Missing required columns in Referential Integrity Check sheet: {required_cols}\")\n            return\n    \n        results = []\n    \n        for _, row in self.excel_df.iterrows():\n            parent_table = str(row[\"parent_table\"]).strip()\n            parent_column = str(row[\"parent_column\"]).strip()\n            child_table = str(row[\"child_table\"]).strip()\n            child_column = str(row[\"child_column\"]).strip()\n    \n            if not parent_table or not parent_column or not child_table or not child_column:\n                logging.warning(\"⚠ Skipping row with missing metadata\")\n                continue\n    \n            query = f\"\"\"\n            SELECT a.{child_column}\n            FROM {schema}.{child_table} a\n            LEFT JOIN {schema}.{parent_table} b\n              ON a.{child_column} = b.{parent_column}\n            WHERE b.{parent_column} IS NULL\n              AND a.{child_column} IS NOT NULL;\n            \"\"\"\n    \n            cursor = self.db.conn.cursor()\n            cursor.execute(query)\n            rows = cursor.fetchall()\n            columns = [col[0] for col in cursor.description]\n            cursor.close()\n    \n            invalid_rows = [dict(zip(columns, r)) for r in rows]\n            invalid_count = len(invalid_rows)\n    \n            results.append({\n                \"Database\": self.db.database,\n                \"Parent_Table\": parent_table,\n                \"Parent_Column\": parent_column,\n                \"Child_Table\": child_table,\n                \"Child_Column\": child_column,\n                \"Invalid_Count\": invalid_count,\n                \"IsCheckPassed\": \"PASS\" if invalid_count == 0 else \"FAIL\",\n                \"Details\": invalid_rows if invalid_count > 0 else None\n            })\n    \n            logging.info(f\"RI Check: {child_table}.{child_column} → {parent_table}.{parent_column} | Invalid = {invalid_count}\")\n    \n        # Save Report\n        report_file = self.report_helper.save_report(results, test_type=\"Referential_Integrity_Report\")\n    \n        # Excel Output\n        summary_df = pd.DataFrame(results)\n        with pd.ExcelWriter(report_file, engine=\"openpyxl\", mode=\"w\") as writer:\n            summary_df.drop(columns=[\"Details\"], errors=\"ignore\").to_excel(writer, sheet_name=\"Summary\", index=False)\n    \n            for r in results:\n                if r.get(\"Invalid_Count\", 0) > 0 and r.get(\"Details\") is not None:\n                    details_df = pd.DataFrame(r[\"Details\"])\n                    if not details_df.empty:\n                        sheet_name = f\"{r['Child_Table']}_FKCheck\"[:31]\n                        details_df.to_excel(writer, sheet_name=sheet_name, index=False)\n    \n>       assert all(r[\"IsCheckPassed\"] == \"PASS\" for r in results), \"❌ Referential Integrity checks failed. See report.\"\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       AssertionError: ❌ Referential Integrity checks failed. See report.\n\nsrc\\Referential_Integrity_validation.py:92: AssertionError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"_session_faker","time":{"start":1759842538097,"stop":1759842538197,"duration":100},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false},{"name":"config_loader","time":{"start":1759842547375,"stop":1759842547390,"duration":15},"status":"passed","steps":[],"attachments":[],"parameters":[],"attachmentStep":false,"hasContent":false,"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false}],"testStage":{"status":"failed","statusMessage":"AssertionError: ❌ Referential Integrity checks failed. See report.","statusTrace":"config_loader = <utils.config_loader.ConfigLoader object at 0x000001BE3CB6EBA0>\n\n    @pytest.mark.skipif(not should_run(\"Referential_Integrity_validation\"), reason=\"Marked N in Excel\")\n    @pytest.mark.not_for_source\n    def test_ReferentialIntegrity_validation(config_loader):\n        validator = ReferentialIntegrity_Validation(config_loader)\n>       validator.run()\n\ntests\\test_smoke_suite.py:109: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <src.Referential_Integrity_validation.ReferentialIntegrity_Validation object at 0x000001BE3CB6F230>\nschema = 'dbo'\n\n    def run(self, schema=\"dbo\"):\n        if self.excel_df is None or self.excel_df.empty:\n            logging.error(\"❌ Referential Integrity Check sheet is missing or empty in Excel.\")\n            return\n    \n        required_cols = {\"parent_table\", \"parent_column\", \"child_table\", \"child_column\"}\n        if not required_cols.issubset(set(self.excel_df.columns.str.lower())):\n            logging.error(f\"❌ Missing required columns in Referential Integrity Check sheet: {required_cols}\")\n            return\n    \n        results = []\n    \n        for _, row in self.excel_df.iterrows():\n            parent_table = str(row[\"parent_table\"]).strip()\n            parent_column = str(row[\"parent_column\"]).strip()\n            child_table = str(row[\"child_table\"]).strip()\n            child_column = str(row[\"child_column\"]).strip()\n    \n            if not parent_table or not parent_column or not child_table or not child_column:\n                logging.warning(\"⚠ Skipping row with missing metadata\")\n                continue\n    \n            query = f\"\"\"\n            SELECT a.{child_column}\n            FROM {schema}.{child_table} a\n            LEFT JOIN {schema}.{parent_table} b\n              ON a.{child_column} = b.{parent_column}\n            WHERE b.{parent_column} IS NULL\n              AND a.{child_column} IS NOT NULL;\n            \"\"\"\n    \n            cursor = self.db.conn.cursor()\n            cursor.execute(query)\n            rows = cursor.fetchall()\n            columns = [col[0] for col in cursor.description]\n            cursor.close()\n    \n            invalid_rows = [dict(zip(columns, r)) for r in rows]\n            invalid_count = len(invalid_rows)\n    \n            results.append({\n                \"Database\": self.db.database,\n                \"Parent_Table\": parent_table,\n                \"Parent_Column\": parent_column,\n                \"Child_Table\": child_table,\n                \"Child_Column\": child_column,\n                \"Invalid_Count\": invalid_count,\n                \"IsCheckPassed\": \"PASS\" if invalid_count == 0 else \"FAIL\",\n                \"Details\": invalid_rows if invalid_count > 0 else None\n            })\n    \n            logging.info(f\"RI Check: {child_table}.{child_column} → {parent_table}.{parent_column} | Invalid = {invalid_count}\")\n    \n        # Save Report\n        report_file = self.report_helper.save_report(results, test_type=\"Referential_Integrity_Report\")\n    \n        # Excel Output\n        summary_df = pd.DataFrame(results)\n        with pd.ExcelWriter(report_file, engine=\"openpyxl\", mode=\"w\") as writer:\n            summary_df.drop(columns=[\"Details\"], errors=\"ignore\").to_excel(writer, sheet_name=\"Summary\", index=False)\n    \n            for r in results:\n                if r.get(\"Invalid_Count\", 0) > 0 and r.get(\"Details\") is not None:\n                    details_df = pd.DataFrame(r[\"Details\"])\n                    if not details_df.empty:\n                        sheet_name = f\"{r['Child_Table']}_FKCheck\"[:31]\n                        details_df.to_excel(writer, sheet_name=sheet_name, index=False)\n    \n>       assert all(r[\"IsCheckPassed\"] == \"PASS\" for r in results), \"❌ Referential Integrity checks failed. See report.\"\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       AssertionError: ❌ Referential Integrity checks failed. See report.\n\nsrc\\Referential_Integrity_validation.py:92: AssertionError","steps":[],"attachments":[{"uid":"4d684b4a19bafdc1","name":"stdout","source":"4d684b4a19bafdc1.txt","type":"text/plain","size":103}],"parameters":[],"attachmentStep":false,"hasContent":true,"stepsCount":0,"attachmentsCount":1,"shouldDisplayMessage":true},"afterStages":[],"labels":[{"name":"tag","value":"not_for_source"},{"name":"parentSuite","value":"tests"},{"name":"suite","value":"test_smoke_suite"},{"name":"host","value":"INATS-LTP-066"},{"name":"thread","value":"37152-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"tests.test_smoke_suite"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":true,"retry":true,"extra":{"categories":[],"tags":["not_for_source"]},"source":"bc4b7848f082175f.json","parameterValues":[]}